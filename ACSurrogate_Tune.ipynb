{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning using Keras tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Jerol Soibam, Dimitra-Eirini Diamantidou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# Tensoflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Kera tuner libraries\n",
    "from keras import backend as K\n",
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner import Objective\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# Data preprocessing\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed state\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"Data//\"\n",
    "\n",
    "file_name = \"data.csv\"\n",
    "data = pd.read_csv(path_train + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR for each column\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the data to exclude outliers\n",
    "data_no_outliers = data[~((data < lower_bound) | (data > upper_bound)).any(axis=1)]\n",
    "\n",
    "# Split the dataset between features (X) and targets (y)\n",
    "X = data_no_outliers.drop(['Unnamed: 0'], axis = 1)[['cell_specific_energy', 'pack_factor', 'eta_batt',\n",
    "                                                     'SP_batt', 'SP_motor', 'eta_motor', 'SP_gen',\n",
    "                                                     'eta_gen', 'eta_splitter']]\n",
    "y = data_no_outliers.drop(['Unnamed: 0'], axis = 1)[['MTOW', \"Block_fuel\", \"El_energy\", \"Block_energy\"]]\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "x_scaled = x_scaler.fit_transform(X.values)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_scaled = y_scaler.fit_transform(y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize sampling space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input space\n",
    "# scatter_matrix = sns.pairplot(X, diag_kind='hist')\n",
    "# scatter_matrix.fig.suptitle('Scatter Plot Matrix', y=1.02)\n",
    "\n",
    "# plot_filename = f'plots//Xscatter_plot.png'\n",
    "# scatter_matrix.savefig(plot_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output space\n",
    "# scatter_matrix = sns.pairplot(y, diag_kind='hist')\n",
    "# scatter_matrix.fig.suptitle('Scatter Plot Matrix', y=1.02)\n",
    "\n",
    "# plot_filename = f'plots//yscatter_plot.png'\n",
    "# scatter_matrix.savefig(plot_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled,\n",
    "                                                    y_scaled, \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data scalers in order to use them when employing the surrogate model \n",
    "joblib.dump(x_scaler, 'x_scaler.pkl')\n",
    "joblib.dump(y_scaler, 'y_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the NN and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    num_layers = hyperparams.Int(\"num_layers\", 2, 6)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        model.add(layers.Dense(\n",
    "            units=hyperparams.Int(f\"units_{i+1}\", 64, 512, step=32),\n",
    "            activation=hyperparams.Choice(f\"act_{i+1}\", [\"relu\", \"tanh\"]),\n",
    "            kernel_regularizer=regularizers.l2(hyperparams.Float(f\"l2_{i+1}\", 1e-5, 1e-1, sampling=\"log\"))\n",
    "        ))\n",
    "\n",
    "    model.add(layers.Dense(y_train.shape[1],))\n",
    "\n",
    "    batch_size = hyperparams.Int(\"batch_size\", 16, 128, step=16)\n",
    "    hp_learning_rate = hyperparams.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    optimizers_dict = {\n",
    "        \"Adam\": keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "        \"RMSprop\": keras.optimizers.legacy.RMSprop(learning_rate=hp_learning_rate)\n",
    "    }\n",
    "\n",
    "    hp_optimizers = hyperparams.Choice('optimizer', values=[\"Adam\", \"RMSprop\"])\n",
    "\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=optimizers_dict[hp_optimizers],\n",
    "        metrics=[\"mean_squared_error\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for writing checkpoints and saving the weights\n",
    "path_checkpoint = 'weights//hyperparameter_search'\n",
    "callback_checpoint = ModelCheckpoint(filepath=path_checkpoint, \n",
    "                                    monitor ='val_loss',\n",
    "                                    verbose=1,\n",
    "                                    save_weights_only = True,\n",
    "                                    save_best_only = True)\n",
    "\n",
    "# Callback for early stopping the optimization when performance worsens on the validation-set\n",
    "callback_early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                                       patience =60,\n",
    "                                       verbose=1)\n",
    "\n",
    "# Callback for TensorBoard log during training\n",
    "tensorboard_log_dir = \"./random_search/tb1_logs\"\n",
    "os.makedirs(tensorboard_log_dir, exist_ok=True)\n",
    "\n",
    "callback_tensorboard = TensorBoard(tensorboard_log_dir)\n",
    "\n",
    "# Callback for learning rate\n",
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                      factor =0.1,\n",
    "                                      min_lr = 1e-5,\n",
    "                                      patience=30,\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "            callback_checpoint,\n",
    "            callback_tensorboard,\n",
    "            callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner1 = RandomSearch(\n",
    "    hypermodel=build_model,\n",
    "    objective=Objective(name=\"val_mean_squared_error\", direction=\"min\"),\n",
    "    max_trials=15,\n",
    "    project_name=\"Regression\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "tuner1.search(X_train, y_train, epochs=2000, validation_split = 0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = tuner1.get_best_hyperparameters()\n",
    "best_params[0].values\n",
    "\n",
    "best_model = tuner1.get_best_models()[0]\n",
    "best_model.summary()\n",
    "\n",
    "best_model.save('surrogate_model.keras', save_format='tf', signatures=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "y_pred_inverse = y_scaler.inverse_transform(y_pred)\n",
    "y_test_inverse = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "ynames = ['MTOW', \"Block_fuel\", \"El_energy\", \"Block_energy\"]\n",
    "ylen = len(ynames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression plots\n",
    "for element in range(ylen):\n",
    "\n",
    "    plt.figure(figsize = [5,5])\n",
    "    plt.scatter(y_test_inverse[:,element], y_pred_inverse[:,element], alpha=0.3)\n",
    "    plt.plot([np.min(y_test_inverse[:,element]),np.max(y_test_inverse[:,element])],[np.min(y_test_inverse[:,element]),np.max(y_test_inverse[:,element])], \"k-\")\n",
    "    plt.xlabel(f\"{ynames[element]} True\")\n",
    "    plt.ylabel(f\"{ynames[element]} Pred\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots//testing//{ynames[element]}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 score\n",
    "for element in range(ylen):\n",
    "    r2 = (r2_score(y_test_inverse[:,element],y_pred_inverse[:,element]))\n",
    "    print(f\"R2 for {ynames[element]} is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "for element in range(ylen):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_inverse[:,element],y_pred_inverse[:,element]))\n",
    "    print(f\"RMSE for {ynames[element]} is {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
